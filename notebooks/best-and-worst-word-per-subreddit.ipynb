{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "bucket_name = 'nr-wsip'\n",
    "\n",
    "index_output_keys = [obj.key for obj in s3.Bucket(bucket_name).objects.filter(Prefix=\"index_output/\")]\n",
    "data_output_keys = [obj.key for obj in s3.Bucket(bucket_name).objects.filter(Prefix=\"data_output/\")]\n",
    "\n",
    "def to_dates(keys):\n",
    "    dates = []\n",
    "    for key in keys:\n",
    "        parts = key.split('/')\n",
    "        if (parts[1] == \"\"):\n",
    "            continue\n",
    "        dates.append(parts[1])\n",
    "    return set(dates)\n",
    "\n",
    "index_output_times = to_dates(index_output_keys)\n",
    "latest_index_output_time = max(index_output_times)\n",
    "\n",
    "data_output_times = to_dates(data_output_keys)\n",
    "latest_data_output_time = max(data_output_times)\n",
    "\n",
    "def to_latest_output_key(latest_output_time, output_keys):\n",
    "    for key in output_keys:\n",
    "        parts = key.split('/')\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        if (parts[1] == latest_output_time) & (parts[2] != '_SUCCESS') & (parts[2] != 'manifest'):\n",
    "            return key\n",
    "        \n",
    "latest_index_output_key = to_latest_output_key(latest_index_output_time, index_output_keys)\n",
    "latest_data_output_key = to_latest_output_key(latest_data_output_time, data_output_keys)\n",
    "\n",
    "\n",
    "def to_ts(unix_str):\n",
    "    return pd.Timestamp(int(unix_str), unit='s')\n",
    "\n",
    "def parse_index_row(row):\n",
    "    output = {}\n",
    "    row_dict = json.loads(row)\n",
    "    output['created_utc'] = to_ts(row_dict['created_utc']['n'])\n",
    "    output['last_updated'] = to_ts(row_dict['last_updated']['n'])\n",
    "    output['day'] = row_dict['day']['s']\n",
    "    output['id'] = row_dict['id']['s']\n",
    "    output['score'] = int(row_dict['score']['n'])\n",
    "    return output\n",
    "\n",
    "def get_string_prop(prop, row):\n",
    "    return row[prop]['s']\n",
    "\n",
    "def parse_data_row(row):\n",
    "    row_dict = json.loads(row)\n",
    "    props = ['id', 'author', 'title', 'permalink', 'selftext', 'subreddit']\n",
    "    return {prop: get_string_prop(prop, row_dict) for prop in props}\n",
    "    \n",
    "\n",
    "def get_s3_obj(bucket_name, key):\n",
    "    return s3.Object(bucket_name, key).get()['Body'].read().decode('utf-8')\n",
    "    \n",
    "def index_obj_to_df(index_obj):\n",
    "    rows = index_obj.split(\"\\n\")[:-1]\n",
    "    output_json = [parse_index_row(row) for row in rows]\n",
    "    return pd.DataFrame(output_json)\n",
    "\n",
    "def data_obj_to_df(data_obj):\n",
    "    rows = data_obj.split(\"\\n\")[:-1]\n",
    "    output_json = [parse_data_row(row) for row in rows]\n",
    "    return pd.DataFrame(output_json)\n",
    "\n",
    "    \n",
    "index = index_obj_to_df(get_s3_obj(bucket_name, latest_index_output_key))\n",
    "data = data_obj_to_df(get_s3_obj(bucket_name, latest_data_output_key))\n",
    "posts = index.join(data.set_index('id'), on='id')\n",
    "posts = posts.assign(alive_time=(posts['last_updated'] - posts['created_utc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = posts['subreddit'].unique()\n",
    "\n",
    "result = []\n",
    "for sub in subs:\n",
    "    sub_posts = posts[posts['subreddit'] == sub]\n",
    "    \n",
    "    sub_posts = sub_posts[sub_posts['alive_time'] > pd.Timedelta(hours=4)]\n",
    "    lower = sub_posts['score'].quantile(0.33)\n",
    "    upper = sub_posts['score'].quantile(0.66)\n",
    "    if len(sub_posts) > 500:\n",
    "        result.append({\"sub\": sub, \"lower\": lower, \"upper\": upper, \"count\": len(sub_posts)})\n",
    "#     print(\"sub: {0} -- lower: {1} -- upper: {2}\".format(sub, lower, upper))\n",
    "\n",
    "popular_subs_summary = pd.DataFrame(result).sort_values(by=\"lower\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_subs = popular_subs_summary['sub'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in popular_subs:\n",
    "    diffs = get_diffs_for_sub(sub, 4)\n",
    "    print(sub, diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "def get_diffs_for_sub(sub, min_alive_hours):\n",
    "\n",
    "    def classif_sub_posts(score):\n",
    "        if score >= upper:\n",
    "            return True\n",
    "        elif score < lower:\n",
    "            return False\n",
    "        else:\n",
    "            return\n",
    "    sub_posts = posts[posts['subreddit'] == sub]\n",
    "    sub_posts = sub_posts[sub_posts['alive_time'] >= pd.Timedelta(hours=min_alive_hours)]\n",
    "    lower = max([sub_posts['score'].quantile(0.33), 2])\n",
    "    upper = max([sub_posts['score'].quantile(0.33), 2])\n",
    "    \n",
    "\n",
    "    sub_posts = sub_posts.assign(good=sub_posts['score'].apply(classif_sub_posts))\n",
    "    sub_posts = sub_posts.dropna(subset=['good'])\n",
    "    \n",
    "    def to_message(row):\n",
    "        is_good = row['good']\n",
    "        words = \"{0} {1}\".format(row['title'], row['selftext'])\n",
    "        return (words, is_good)\n",
    "    \n",
    "    messages = [to_message(post) for _, post in sub_posts.iterrows()]\n",
    "    \n",
    "    word_probs = messages_to_word_probabilities(messages)\n",
    "    \n",
    "    words_good = [(word, p_good) for word, p_good, p_bad in word_probs]\n",
    "    words_bad = [(word, p_bad) for word, p_good, p_bad in word_probs]\n",
    "    \n",
    "    def to_series(word_tups):\n",
    "        words, scores = zip(*word_tups)\n",
    "        return pd.Series(scores, index=words)\n",
    "    \n",
    "#     best = to_series(words_good).sort_values(ascending=False).head(100)\n",
    "#     worst = to_series(words_bad).sort_values(ascending=False).head(100)\n",
    "    \n",
    "#     best_index_unique = np.setdiff1d(best.index, worst.index)\n",
    "#     best = best[best_index_unique].sort_values(ascending=False).head(10)\n",
    "    \n",
    "#     worst_index_unique = np.setdiff1d(worst.index, best.index)\n",
    "#     worst = worst[worst_index_unique].sort_values(ascending=False).head(10)\n",
    "#     diffs = word_diffs(messages)\n",
    "\n",
    "    diffs = to_series(words_good).sort_values(ascending=False)\n",
    "    best = diffs.head(10)\n",
    "    worst = diffs.tail(10).sort_values()\n",
    "    return best\n",
    "\n",
    "\n",
    "def tokenize(message):\n",
    "    stops = ['our', 'their', 'your', 'one', 'about', \"i'm\", 'up', 'out', 'am', 'any', 'like', 'when', 'now', 'her', \"it's\",  'we', 'us', 'they', 'he', 'she', 'his', 'u', 'com', 'http', 'https', 'www', 'or', 'who', 'would', 'had', 'any' 'at', 'got', 'l', 'my', 'me', 'does','get', 'were', 'what', 'at', 'too', 'as', 's', 'an', 'than', 'do', 'so', 'no', 'it', 'how', 'be', 'has',  'a','can', 'will', 'have', 'if', 'why', 'but','he', 'the', 'and', 'on', 'all', 'is', 'emptystring', 'are', 'said', 'in', 'of', 'just', 'that', 'i', 'with', 'was', 'r', 't', 'to', 'for', 'by', 'you', 'there', 'not', 'to', 'from', 'this', '0' , '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    filt_words = list(filter(lambda x: x not in stops, all_words))\n",
    "    \n",
    "    return set(filt_words)                          # remove duplicates\n",
    "\n",
    "\n",
    "def count_words(messages):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_good in messages:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_good else 1] += 1\n",
    "    return counts\n",
    "\n",
    "def word_probabilities(counts, total_goodposts, total_non_goodposts, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets\n",
    "    w, p(w | good) and p(w | ~good)\"\"\"\n",
    "    return [(w,\n",
    "             (good + k) / (total_goodposts + 2 * k),\n",
    "             (non_good + k) / (total_non_goodposts + 2 * k))\n",
    "             for w, (good, non_good) in counts.items()]\n",
    "\n",
    "def messages_to_word_probabilities(messages):\n",
    "    num_goodposts = len([is_good\n",
    "                     for message, is_good in messages\n",
    "                     if is_good])\n",
    "    num_non_goodposts = len(messages) - num_goodposts\n",
    "\n",
    "    # run training data through our \"pipeline\"\n",
    "    k = 0.5\n",
    "    word_counts = count_words(messages)\n",
    "    word_probs = word_probabilities(word_counts,\n",
    "                                         num_goodposts,\n",
    "                                         num_non_goodposts,\n",
    "                                         k)\n",
    "    \n",
    "    return word_probs\n",
    "#     words_and_diffs = [(word, (p_good - p_bad)) for word, p_good, p_bad in word_probs]\n",
    "    \n",
    "    # try just the p_good\n",
    "#     words_and_diffs = [(word, p_good) for word, p_good, p_bad in word_probs]\n",
    "\n",
    "#     if len(words_and_diffs) == 0:\n",
    "#         return pd.Series()\n",
    "#     words, diffs = zip(*words_and_diffs)\n",
    "#     diffs = pd.Series(diffs, index=words).sort_values(ascending=False)\n",
    "#     return diffs.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sh_items = posts[posts['subreddit'] == 'soccer']\n",
    "min_alive_time = 8\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scores = sh_items[sh_items['alive_time'] > pd.Timedelta(hours=min_alive_time)]['score']\n",
    "scores[scores < 51].hist(bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_times = sh_items['created_utc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_times.iloc[0].minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "created_times = sh_items['created_utc']\n",
    "\n",
    "created_minutes = pd.Series(created_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = created_minutes.hist(bins=48)\n",
    "ax.set_xlim([0,1441])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_time_data(subreddit_name):\n",
    "\n",
    "    def to_minute_of_day(timestamp):\n",
    "        return (timestamp.hour * 60) + timestamp.minute\n",
    "\n",
    "    sub_posts = posts[posts[\"subreddit\"] == subreddit_name]\n",
    "    sub_posts = sub_posts[sub_posts['alive_time'] > pd.Timedelta(hours=4)]\n",
    "    created_times = sub_posts['created_utc']\n",
    "    created_minutes = [to_minute_of_day(t) for _, t in created_times.items()]\n",
    "\n",
    "    bins = [30 * n for n in range(49)]\n",
    "    group_names = range(48)\n",
    "    time_bins = pd.cut(created_minutes, bins, labels=group_names)\n",
    "    sub_posts = sub_posts.assign(time_bin=time_bins)\n",
    "\n",
    "#     bin_results = {'bin': [], 'med': [], 'upper': [], 'lower': [], 'count':[]}\n",
    "    bin_results = {'bin': [], 'med': [], 'count':[]}    \n",
    "    for t in group_names:\n",
    "        bin_posts = sub_posts[sub_posts[\"time_bin\"] == t]\n",
    "        bin_med_score = bin_posts['score'].median()\n",
    "        bin_upper_score = bin_med_score + bin_posts['score'].std()\n",
    "        bin_lower_score = bin_med_score + bin_posts['score'].std()\n",
    "        bin_count = bin_posts['score'].count()\n",
    "        bin_results['bin'].append(t)\n",
    "        bin_results['med'].append(bin_med_score)\n",
    "#         bin_results['upper'].append(bin_upper_score)\n",
    "#         bin_results['lower'].append(bin_lower_score)\n",
    "        bin_results['count'].append(bin_count)\n",
    "    \n",
    "    return pd.DataFrame(bin_results).set_index('bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subreddit_time_data('aww').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.value_counts().sort_index().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
