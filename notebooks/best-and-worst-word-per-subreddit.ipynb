{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "indexTable = dynamodb.Table('nr-wsip-posts_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = indexTable.query(\n",
    "    KeyConditionExpression=Key('day').eq('2017-05-20')\n",
    ")\n",
    "items_5_20 = response['Items']\n",
    "\n",
    "response = indexTable.query(\n",
    "    KeyConditionExpression=Key('day').eq('2017-05-21')\n",
    ")\n",
    "items_5_21 = response['Items']\n",
    "\n",
    "items = items_5_20 + items_5_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_decimals(item):\n",
    "    dec_fields = ['created_utc', 'last_updated', 'score']\n",
    "    for f in dec_fields:\n",
    "        item[f] = int(item[f])\n",
    "    return item\n",
    "    \n",
    "items = [parse_decimals(i) for i in items]\n",
    "items = pd.DataFrame(items)\n",
    "\n",
    "date_fields = ['created_utc', 'last_updated']\n",
    "\n",
    "for d in date_fields:\n",
    "    items[d] = items[d].map(lambda x: pd.Timestamp(x, unit='s'))\n",
    "    \n",
    "items = items.assign(alive_time=(items['last_updated'] - items['created_utc']))\n",
    "\n",
    "dataTable = dynamodb.Table('nr-wsip-posts_data')\n",
    "\n",
    "def get_data_item(id):\n",
    "    try:\n",
    "        response = dataTable.get_item(\n",
    "            Key={\n",
    "                'id': id\n",
    "            }\n",
    "        )\n",
    "        data_item = response['Item']\n",
    "        return data_item\n",
    "    except:\n",
    "#         print(\"bad response for id {0}\".format(id))\n",
    "        return {}\n",
    "\n",
    "data = [get_data_item(id) for _, id in items['id'].iteritems()]\n",
    "data = pd.DataFrame(data).set_index('id')\n",
    "\n",
    "items = items.join(data, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sh_items = items[items['subreddit'] == 'Showerthoughts']\n",
    "min_alive_time = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1172669b0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDHRJKsJAl6QiDHRJKsJAl6Qi/h8ufNqyA0R8vgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117260f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scores = sh_items[sh_items['alive_time'] > pd.Timedelta(hours=min_alive_time)]['score']\n",
    "scores[scores < 51].hist(bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_alive_time = 12\n",
    "\n",
    "counts = items[items['alive_time'] >= pd.Timedelta(hours=min_alive_time)].groupby(\"subreddit\").count()['score'].sort_values(ascending=False)\n",
    "med_scores = items.groupby(\"subreddit\").median()['score'].sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "AskReddit             2816\n",
       "The_Donald            2514\n",
       "Showerthoughts         700\n",
       "funny                  693\n",
       "aww                    638\n",
       "pics                   609\n",
       "Overwatch              544\n",
       "leagueoflegends        508\n",
       "me_irl                 493\n",
       "politics               419\n",
       "videos                 419\n",
       "DotA2                  373\n",
       "gaming                 331\n",
       "mildlyinteresting      321\n",
       "Jokes                  319\n",
       "pcmasterrace           309\n",
       "SquaredCircle          308\n",
       "worldnews              299\n",
       "soccer                 269\n",
       "nba                    265\n",
       "dankmemes              258\n",
       "GlobalOffensive        249\n",
       "DestinyTheGame         231\n",
       "relationships          205\n",
       "rupaulsdragrace        204\n",
       "PrequelMemes           198\n",
       "conspiracy             197\n",
       "hiphopheads            157\n",
       "movies                 155\n",
       "gifs                   148\n",
       "todayilearned          146\n",
       "anime                  141\n",
       "BlackPeopleTwitter     109\n",
       "news                   107\n",
       "OldSchoolCool          106\n",
       "MarchAgainstTrump       98\n",
       "WTF                     78\n",
       "CringeAnarchy           78\n",
       "europe                  72\n",
       "hockey                  71\n",
       "baseball                67\n",
       "MMA                     60\n",
       "AdviceAnimals           60\n",
       "neoliberal              52\n",
       "tifu                    48\n",
       "technology              46\n",
       "nfl                     42\n",
       "nottheonion             41\n",
       "SubredditDrama          34\n",
       "OutOfTheLoop            27\n",
       "Documentaries           12\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "def get_diffs_for_sub(sub, min_alive_hours, non_good_max, good_min):\n",
    "    def classif_sub_posts(score):\n",
    "        if score >= good_min:\n",
    "            return True\n",
    "        elif score < non_good_max:\n",
    "            return False\n",
    "        else:\n",
    "            return \n",
    "    sub_posts = items[items['subreddit'] == sub]\n",
    "    sub_posts = sub_posts[sub_posts['alive_time'] >= pd.Timedelta(hours=min_alive_hours)]\n",
    "    sub_posts = sub_posts.assign(good=sub_posts['score'].apply(classif_sub_posts))\n",
    "    sub_posts = sub_posts.dropna(subset=['good'])\n",
    "    \n",
    "    def to_message(row):\n",
    "        is_good = row['good']\n",
    "        words = \"{0} {1}\".format(row['title'], row['selftext'])\n",
    "        return (words, is_good)\n",
    "    \n",
    "    messages = [to_message(post) for _, post in sub_posts.iterrows()]\n",
    "    \n",
    "    word_probs = messages_to_word_probabilities(messages)\n",
    "    \n",
    "    words_good = [(word, p_good) for word, p_good, p_bad in word_probs]\n",
    "    words_bad = [(word, p_bad) for word, p_good, p_bad in word_probs]\n",
    "    \n",
    "    def to_series(word_tups):\n",
    "        words, scores = zip(*word_tups)\n",
    "        return pd.Series(scores, index=words)\n",
    "    \n",
    "#     best = to_series(words_good).sort_values(ascending=False).head(100)\n",
    "#     worst = to_series(words_bad).sort_values(ascending=False).head(100)\n",
    "    \n",
    "#     best_index_unique = np.setdiff1d(best.index, worst.index)\n",
    "#     best = best[best_index_unique].sort_values(ascending=False).head(10)\n",
    "    \n",
    "#     worst_index_unique = np.setdiff1d(worst.index, best.index)\n",
    "#     worst = worst[worst_index_unique].sort_values(ascending=False).head(10)\n",
    "#     diffs = word_diffs(messages)\n",
    "\n",
    "    diffs = to_series(words_good).sort_values(ascending=False)\n",
    "    best = diffs.head(10)\n",
    "    worst = diffs.tail(10).sort_values()\n",
    "    return {\"best\": best, \"worst\": worst }\n",
    "\n",
    "\n",
    "def tokenize(message):\n",
    "    stops = ['or', 'who', 'would', 'had', 'any' 'at', 'got', 'l', 'my', 'me', 'does','get', 'were', 'what', 'at', 'too', 'as', 's', 'an', 'than', 'do', 'so', 'no', 'it', 'how', 'be', 'has',  'a','can', 'will', 'have', 'if', 'why', 'but','he', 'the', 'and', 'on', 'all', 'is', 'emptystring', 'are', 'said', 'in', 'of', 'just', 'that', 'i', 'with', 'was', 'r', 't', 'to', 'for', 'by', 'you', 'there', 'not', 'to', 'from', 'this', '0' , '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    filt_words = list(filter(lambda x: x not in stops, all_words))\n",
    "    \n",
    "    return set(filt_words)                          # remove duplicates\n",
    "\n",
    "\n",
    "def count_words(messages):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_good in messages:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_good else 1] += 1\n",
    "    return counts\n",
    "\n",
    "def word_probabilities(counts, total_goodposts, total_non_goodposts, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets\n",
    "    w, p(w | good) and p(w | ~good)\"\"\"\n",
    "    return [(w,\n",
    "             (good + k) / (total_goodposts + 2 * k),\n",
    "             (non_good + k) / (total_non_goodposts + 2 * k))\n",
    "             for w, (good, non_good) in counts.items()]\n",
    "\n",
    "def messages_to_word_probabilities(messages):\n",
    "    num_goodposts = len([is_good\n",
    "                     for message, is_good in messages\n",
    "                     if is_good])\n",
    "    num_non_goodposts = len(messages) - num_goodposts\n",
    "\n",
    "    # run training data through our \"pipeline\"\n",
    "    k = 0.5\n",
    "    word_counts = count_words(messages)\n",
    "    word_probs = word_probabilities(word_counts,\n",
    "                                         num_goodposts,\n",
    "                                         num_non_goodposts,\n",
    "                                         k)\n",
    "    \n",
    "    return word_probs\n",
    "#     words_and_diffs = [(word, (p_good - p_bad)) for word, p_good, p_bad in word_probs]\n",
    "    \n",
    "    # try just the p_good\n",
    "#     words_and_diffs = [(word, p_good) for word, p_good, p_bad in word_probs]\n",
    "\n",
    "#     if len(words_and_diffs) == 0:\n",
    "#         return pd.Series()\n",
    "#     words, diffs = zip(*words_and_diffs)\n",
    "#     diffs = pd.Series(diffs, index=words).sort_values(ascending=False)\n",
    "#     return diffs.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
