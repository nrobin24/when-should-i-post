{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'nr-wsip'\n",
    "\n",
    "index_prefix = \"index_output/\"\n",
    "data_prefix = \"data_output/\"\n",
    "min_alive_hours = 4\n",
    "\n",
    "index_df = s3_to_df(bucket_name, index_prefix, parse_index_row)\n",
    "data_df = s3_to_df(bucket_name, data_prefix, parse_data_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = index_df.join(data_df.set_index('id'), on='id')\n",
    "# posts = posts.assign(alive_time=(posts['last_updated'] - posts['created_utc']))\n",
    "\n",
    "# posts = posts[posts['alive_time'] > pd.Timedelta(hours=min_alive_hours)]\n",
    "\n",
    "subs = posts['subreddit'].unique()\n",
    "\n",
    "sub_posts = {sub: posts[posts[\"subreddit\"] == sub] for sub in subs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posts_to_data(posts):\n",
    "    return {\n",
    "        \"time_of_day\": get_time_of_day_data(posts),\n",
    "        \"words\": get_word_probs(posts)\n",
    "    }\n",
    "\n",
    "sub_data = {sub: posts_to_data(s_posts) for sub, s_posts in sub_posts.items()}\n",
    "with open('sub_data.json', 'w') as outfile:\n",
    "    json.dump(sub_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319984   2017-05-25 14:45:30\n",
       "320034   2017-05-25 14:45:31\n",
       "320084   2017-05-25 14:45:35\n",
       "320484   2017-05-25 14:45:54\n",
       "322034   2017-05-25 14:47:15\n",
       "322084   2017-05-25 14:47:16\n",
       "322434   2017-05-25 14:47:25\n",
       "322784   2017-05-25 14:47:43\n",
       "323184   2017-05-25 14:48:01\n",
       "323434   2017-05-25 14:48:10\n",
       "323534   2017-05-25 14:48:13\n",
       "323834   2017-05-25 14:48:21\n",
       "324084   2017-05-25 14:48:27\n",
       "324234   2017-05-25 14:48:31\n",
       "318835   2017-05-25 14:49:08\n",
       "318885   2017-05-25 14:49:09\n",
       "318985   2017-05-25 14:49:13\n",
       "319285   2017-05-25 14:49:38\n",
       "319685   2017-05-25 14:49:51\n",
       "88022    2017-05-26 00:00:03\n",
       "88122    2017-05-26 00:00:11\n",
       "88422    2017-05-26 00:00:19\n",
       "88472    2017-05-26 00:00:34\n",
       "88722    2017-05-26 00:00:44\n",
       "88822    2017-05-26 00:00:54\n",
       "89722    2017-05-26 00:02:05\n",
       "89872    2017-05-26 00:02:15\n",
       "90072    2017-05-26 00:02:30\n",
       "90122    2017-05-26 00:02:30\n",
       "90572    2017-05-26 00:02:41\n",
       "84823    2017-05-26 00:03:13\n",
       "84873    2017-05-26 00:03:15\n",
       "84923    2017-05-26 00:03:16\n",
       "85073    2017-05-26 00:03:24\n",
       "85173    2017-05-26 00:03:29\n",
       "85273    2017-05-26 00:03:35\n",
       "85323    2017-05-26 00:03:36\n",
       "86123    2017-05-26 00:04:13\n",
       "86823    2017-05-26 00:04:33\n",
       "86923    2017-05-26 00:04:38\n",
       "86973    2017-05-26 00:04:39\n",
       "87423    2017-05-26 00:04:59\n",
       "88273    2017-05-26 00:05:39\n",
       "88423    2017-05-26 00:05:41\n",
       "88823    2017-05-26 00:05:55\n",
       "89773    2017-05-26 00:07:21\n",
       "90173    2017-05-26 00:07:33\n",
       "90273    2017-05-26 00:07:42\n",
       "90423    2017-05-26 00:07:44\n",
       "90673    2017-05-26 00:07:58\n",
       "Name: created_utc, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_posts['The_Donald']['created_utc'].sort_values().iloc[12400:12450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69015    2017-05-24 14:35:18\n",
       "69115    2017-05-24 14:35:19\n",
       "69415    2017-05-24 14:35:32\n",
       "68425    2017-05-24 15:19:40\n",
       "72682    2017-05-24 15:54:24\n",
       "68889    2017-05-24 16:22:44\n",
       "72449    2017-05-24 17:06:30\n",
       "305255   2017-05-25 00:00:02\n",
       "305555   2017-05-25 00:00:23\n",
       "305455   2017-05-25 00:00:23\n",
       "305855   2017-05-25 00:00:35\n",
       "306005   2017-05-25 00:00:36\n",
       "306405   2017-05-25 00:00:47\n",
       "308405   2017-05-25 00:03:06\n",
       "308505   2017-05-25 00:03:07\n",
       "308455   2017-05-25 00:03:07\n",
       "308755   2017-05-25 00:03:23\n",
       "309155   2017-05-25 00:03:34\n",
       "309205   2017-05-25 00:03:35\n",
       "309305   2017-05-25 00:03:40\n",
       "309705   2017-05-25 00:04:13\n",
       "309805   2017-05-25 00:04:16\n",
       "310205   2017-05-25 00:04:32\n",
       "310405   2017-05-25 00:04:34\n",
       "310355   2017-05-25 00:04:34\n",
       "310755   2017-05-25 00:04:44\n",
       "310905   2017-05-25 00:04:46\n",
       "310805   2017-05-25 00:04:46\n",
       "311255   2017-05-25 00:05:11\n",
       "305106   2017-05-25 00:05:23\n",
       "305356   2017-05-25 00:05:28\n",
       "305556   2017-05-25 00:05:32\n",
       "306056   2017-05-25 00:05:53\n",
       "306756   2017-05-25 00:06:39\n",
       "307056   2017-05-25 00:06:52\n",
       "307856   2017-05-25 00:07:25\n",
       "308456   2017-05-25 00:07:44\n",
       "308506   2017-05-25 00:07:50\n",
       "308856   2017-05-25 00:08:07\n",
       "309006   2017-05-25 00:08:18\n",
       "309456   2017-05-25 00:08:32\n",
       "309756   2017-05-25 00:08:43\n",
       "309906   2017-05-25 00:08:50\n",
       "310056   2017-05-25 00:08:57\n",
       "310156   2017-05-25 00:09:05\n",
       "310356   2017-05-25 00:09:11\n",
       "310556   2017-05-25 00:09:19\n",
       "310956   2017-05-25 00:09:35\n",
       "311006   2017-05-25 00:09:38\n",
       "305257   2017-05-25 00:10:06\n",
       "Name: created_utc, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_posts['AskReddit']['created_utc'].sort_values().iloc[11750:11800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bin': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23],\n",
       " 'count': [3564,\n",
       "  3661,\n",
       "  3658,\n",
       "  3495,\n",
       "  3312,\n",
       "  2970,\n",
       "  2814,\n",
       "  2448,\n",
       "  2235,\n",
       "  2242,\n",
       "  2282,\n",
       "  2648,\n",
       "  3159,\n",
       "  3384,\n",
       "  3550,\n",
       "  3661,\n",
       "  3754,\n",
       "  3649,\n",
       "  3515,\n",
       "  3567,\n",
       "  3505,\n",
       "  3505,\n",
       "  3505,\n",
       "  3506],\n",
       " 'med': [2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_time_of_day_data(sub_posts['AskReddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def to_dates(keys):\n",
    "    dates = []\n",
    "    for key in keys:\n",
    "        parts = key.split('/')\n",
    "        if (parts[1] == \"\"):\n",
    "            continue\n",
    "        dates.append(parts[1])\n",
    "    return set(dates)\n",
    "\n",
    "def to_latest_output_keys(latest_output_time, output_keys):\n",
    "    keys = []\n",
    "    for key in output_keys:\n",
    "        parts = key.split('/')\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        if (parts[1] == latest_output_time) & (parts[2] != '_SUCCESS') & (parts[2] != 'manifest'):\n",
    "            keys.append(key)\n",
    "    return keys\n",
    "\n",
    "def get_latest_output_keys(bucket_name, prefix):\n",
    "    output_keys = [obj.key for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix)]\n",
    "    output_times = to_dates(output_keys) \n",
    "    latest_time = max(output_times)\n",
    "    latest_output_keys = to_latest_output_keys(latest_time, output_keys)\n",
    "    return latest_output_keys\n",
    "\n",
    "def s3_to_df(bucket_name, prefix, parser):\n",
    "    keys = get_latest_output_keys(bucket_name, prefix)\n",
    "    output = []\n",
    "    for key in keys:\n",
    "        s3_obj = s3.Object(bucket_name, key).get()['Body'].read().decode('utf-8')\n",
    "        rows = s3_obj.split(\"\\n\")[:-1]\n",
    "        this_output = [parser(row) for row in rows]\n",
    "        output += this_output\n",
    "    return pd.DataFrame(output)\n",
    "\n",
    "def to_ts(unix_str):\n",
    "    return pd.Timestamp(int(unix_str), unit='s')\n",
    "\n",
    "def get_string_prop(prop, row):\n",
    "    return row[prop]['s']\n",
    "\n",
    "def parse_index_row(row):\n",
    "    output = {}\n",
    "    row_dict = json.loads(row)\n",
    "    output['created_utc'] = to_ts(row_dict['created_utc']['n'])\n",
    "    output['last_updated'] = to_ts(row_dict['last_updated']['n'])\n",
    "    output['day'] = row_dict['day']['s']\n",
    "    output['id'] = row_dict['id']['s']\n",
    "    output['score'] = int(row_dict['score']['n'])\n",
    "    return output\n",
    "\n",
    "def parse_data_row(row):\n",
    "    row_dict = json.loads(row)\n",
    "    props = ['id', 'author', 'title', 'permalink', 'selftext', 'subreddit']\n",
    "    return {prop: get_string_prop(prop, row_dict) for prop in props}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_of_day_data(posts):\n",
    "    \n",
    "    \n",
    "    created_times = posts['created_utc']\n",
    "    created_hours = posts['created_utc'].apply(lambda x: x.hour)\n",
    "    posts = posts.assign(hour=created_hours)\n",
    "    bin_results = {'bin': [], 'med': [], 'count':[]}  \n",
    "    for h in range(24):\n",
    "        bin_posts = posts[posts[\"hour\"] == h]\n",
    "        if len(bin_posts):\n",
    "                bin_med_score = bin_posts['score'].median()\n",
    "        else:\n",
    "            bin_med_score = 0\n",
    "        bin_results['bin'].append(h)\n",
    "        bin_results['med'].append(bin_med_score)\n",
    "        bin_results['count'].append(len(bin_posts))\n",
    "    \n",
    "    return bin_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_of_day_data_old(posts):\n",
    "\n",
    "    def to_minute_of_day(timestamp):\n",
    "        return (timestamp.hour * 60) + timestamp.minute\n",
    "    created_times = posts['created_utc']\n",
    "    created_minutes = [to_minute_of_day(t) for _, t in created_times.items()]\n",
    "\n",
    "    bins = [30 * n for n in range(49)]\n",
    "    group_names = range(48)\n",
    "    time_bins = pd.cut(created_minutes, bins, labels=group_names)\n",
    "    posts = posts.assign(time_bin=time_bins)\n",
    "\n",
    "#     bin_results = {'bin': [], 'med': [], 'upper': [], 'lower': [], 'count':[]}\n",
    "    bin_results = {'bin': [], 'med': [], 'count':[]}    \n",
    "    for t in group_names:\n",
    "        bin_posts = posts[posts[\"time_bin\"] == t]\n",
    "        bin_med_score = bin_posts['score'].median()\n",
    "        if len(bin_posts):\n",
    "            bin_med_score = bin_posts['score'].median()\n",
    "        else:\n",
    "            bin_med_score = 0\n",
    "        bin_upper_score = bin_med_score + bin_posts['score'].std()\n",
    "        bin_lower_score = bin_med_score + bin_posts['score'].std()\n",
    "        bin_count = bin_posts['score'].count().item()\n",
    "        bin_results['bin'].append(t)\n",
    "        bin_results['med'].append(bin_med_score)\n",
    "#         bin_results['upper'].append(bin_upper_score)\n",
    "#         bin_results['lower'].append(bin_lower_score)\n",
    "        bin_results['count'].append(bin_count)\n",
    "    \n",
    "    return bin_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def to_message(row):\n",
    "    is_good = row['good']\n",
    "    words = \"{0} {1}\".format(row['title'], row['selftext'])\n",
    "    return (words, is_good)\n",
    "\n",
    "def classify_posts(lower, upper, score):\n",
    "    if score >= upper:\n",
    "        return True\n",
    "    elif score < lower:\n",
    "        return False\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def probs_to_distance(p_good, p_bad):\n",
    "    return (p_good - p_bad)/math.sqrt(2)\n",
    "\n",
    "def get_word_probs(posts):\n",
    "    result = {\n",
    "        \"best\": {\"words\": [], \"probs\": []}, \"worst\": {\"words\": [], \"probs\": []}\n",
    "    }\n",
    "#     lower_score = max([posts['score'].quantile(0.33), 2])\n",
    "    lower_score = 3\n",
    "    upper_score = max([posts['score'].quantile(0.66), 4])\n",
    "    posts = posts.assign(\n",
    "        good=posts['score'].apply(\n",
    "            lambda p: classify_posts(lower_score, upper_score, p)\n",
    "        )).dropna(subset=['good'])\n",
    "    messages = [to_message(post) for _, post in posts.iterrows()]\n",
    "    word_probs = messages_to_word_probabilities(messages)\n",
    "    word_dists = [(word, probs_to_distance(p_good, p_bad)) for\n",
    "                  word, p_good, p_bad in word_probs]\n",
    "\n",
    "    if len(word_dists) == 0:\n",
    "        return result\n",
    "    \n",
    "    words, dists = zip(*word_dists)\n",
    "    word_dists = pd.Series(dists, index=words).sort_values()\n",
    "    worst = word_dists.head(10)\n",
    "    best = word_dists.tail(10).sort_values(ascending=False)\n",
    "    result['best']['words'] = best.index.tolist()\n",
    "    result['best']['probs'] = best.values.tolist()\n",
    "    result['worst']['words'] = worst.index.tolist()\n",
    "    result['worst']['probs'] = worst.values.tolist()\n",
    "    return result\n",
    "\n",
    "    \n",
    "def tokenize(message):\n",
    "    stops = ['our', 'their', 'your', 'one', 'about', \"i'm\", 'up', 'out', 'am', 'any', 'like', 'when', 'now', 'her', \"it's\",  'we', 'us', 'they', 'he', 'she', 'his', 'u', 'com', 'http', 'https', 'www', 'or', 'who', 'would', 'had', 'any' 'at', 'got', 'l', 'my', 'me', 'does','get', 'were', 'what', 'at', 'too', 'as', 's', 'an', 'than', 'do', 'so', 'no', 'it', 'how', 'be', 'has',  'a','can', 'will', 'have', 'if', 'why', 'but','he', 'the', 'and', 'on', 'all', 'is', 'emptystring', 'are', 'said', 'in', 'of', 'just', 'that', 'i', 'with', 'was', 'r', 't', 'to', 'for', 'by', 'you', 'there', 'not', 'to', 'from', 'this', '0' , '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    filt_words = list(filter(lambda x: x not in stops, all_words))\n",
    "    \n",
    "    return set(filt_words)   \n",
    "    \n",
    "def count_words(messages):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_good in messages:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_good else 1] += 1\n",
    "    return counts\n",
    "    \n",
    "def messages_to_word_probabilities(messages):\n",
    "    num_goodposts = len([is_good\n",
    "                     for message, is_good in messages\n",
    "                     if is_good])\n",
    "    num_non_goodposts = len(messages) - num_goodposts\n",
    "\n",
    "    k = 0.5\n",
    "    word_counts = count_words(messages)\n",
    "    word_probs = word_probabilities(word_counts,\n",
    "                                         num_goodposts,\n",
    "                                         num_non_goodposts,\n",
    "                                         k)\n",
    "    return word_probs\n",
    "\n",
    "def word_probabilities(counts, total_goodposts, total_non_goodposts, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets\n",
    "    w, p(w | good) and p(w | ~good)\"\"\"\n",
    "    return [(w,\n",
    "             (good + k) / (total_goodposts + 2 * k),\n",
    "             (non_good + k) / (total_non_goodposts + 2 * k))\n",
    "             for w, (good, non_good) in counts.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
