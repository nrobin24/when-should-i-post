{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'nr-wsip'\n",
    "\n",
    "index_prefix = \"index_output/\"\n",
    "data_prefix = \"data_output/\"\n",
    "min_alive_hours = 4\n",
    "\n",
    "index_df = s3_to_df(bucket_name, index_prefix, parse_index_row)\n",
    "data_df = s3_to_df(bucket_name, data_prefix, parse_data_row)\n",
    "posts = index_df.join(data_df.set_index('id'), on='id')\n",
    "posts = posts.assign(alive_time=(posts['last_updated'] - posts['created_utc']))\n",
    "\n",
    "posts = posts[posts['alive_time'] > pd.Timedelta(hours=min_alive_hours)]\n",
    "\n",
    "subs = posts['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_posts = {sub: posts[posts[\"subreddit\"] == sub] for sub in subs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def to_dates(keys):\n",
    "    dates = []\n",
    "    for key in keys:\n",
    "        parts = key.split('/')\n",
    "        if (parts[1] == \"\"):\n",
    "            continue\n",
    "        dates.append(parts[1])\n",
    "    return set(dates)\n",
    "\n",
    "def to_latest_output_key(latest_output_time, output_keys):\n",
    "    for key in output_keys:\n",
    "        parts = key.split('/')\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        if (parts[1] == latest_output_time) & (parts[2] != '_SUCCESS') & (parts[2] != 'manifest'):\n",
    "            return key\n",
    "\n",
    "def get_latest_output_key(bucket_name, prefix):\n",
    "    output_keys = [obj.key for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix)]\n",
    "    output_times = to_dates(output_keys) \n",
    "    latest_time = max(output_times)\n",
    "    latest_output_key = to_latest_output_key(latest_time, output_keys)\n",
    "    return latest_output_key\n",
    "\n",
    "def s3_to_df(bucket_name, prefix, parser):\n",
    "    key = get_latest_output_key(bucket_name, prefix)\n",
    "    s3_obj = s3.Object(bucket_name, key).get()['Body'].read().decode('utf-8')\n",
    "    rows = s3_obj.split(\"\\n\")[:-1]\n",
    "    output = [parser(row) for row in rows]\n",
    "    return pd.DataFrame(output)\n",
    "\n",
    "def to_ts(unix_str):\n",
    "    return pd.Timestamp(int(unix_str), unit='s')\n",
    "\n",
    "def get_string_prop(prop, row):\n",
    "    return row[prop]['s']\n",
    "\n",
    "def parse_index_row(row):\n",
    "    output = {}\n",
    "    row_dict = json.loads(row)\n",
    "    output['created_utc'] = to_ts(row_dict['created_utc']['n'])\n",
    "    output['last_updated'] = to_ts(row_dict['last_updated']['n'])\n",
    "    output['day'] = row_dict['day']['s']\n",
    "    output['id'] = row_dict['id']['s']\n",
    "    output['score'] = int(row_dict['score']['n'])\n",
    "    return output\n",
    "\n",
    "def parse_data_row(row):\n",
    "    row_dict = json.loads(row)\n",
    "    props = ['id', 'author', 'title', 'permalink', 'selftext', 'subreddit']\n",
    "    return {prop: get_string_prop(prop, row_dict) for prop in props}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def posts_to_data(posts):\n",
    "    return {\n",
    "        \"time_of_day\": get_time_of_day_data(posts),\n",
    "        \"words\": get_word_probs(posts)\n",
    "    }\n",
    "\n",
    "sub_data = {sub: posts_to_data(s_posts) for sub, s_posts in sub_posts.items()}\n",
    "with open('sub_data.json', 'w') as outfile:\n",
    "    json.dump(sub_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_of_day_data(posts):\n",
    "\n",
    "    def to_minute_of_day(timestamp):\n",
    "        return (timestamp.hour * 60) + timestamp.minute\n",
    "    created_times = posts['created_utc']\n",
    "    created_minutes = [to_minute_of_day(t) for _, t in created_times.items()]\n",
    "\n",
    "    bins = [30 * n for n in range(49)]\n",
    "    group_names = range(48)\n",
    "    time_bins = pd.cut(created_minutes, bins, labels=group_names)\n",
    "    posts = posts.assign(time_bin=time_bins)\n",
    "\n",
    "#     bin_results = {'bin': [], 'med': [], 'upper': [], 'lower': [], 'count':[]}\n",
    "    bin_results = {'bin': [], 'med': [], 'count':[]}    \n",
    "    for t in group_names:\n",
    "        bin_posts = posts[posts[\"time_bin\"] == t]\n",
    "        bin_med_score = bin_posts['score'].median()\n",
    "        if len(bin_posts):\n",
    "            bin_med_score = bin_posts['score'].median()\n",
    "        else:\n",
    "            bin_med_score = 0\n",
    "        bin_upper_score = bin_med_score + bin_posts['score'].std()\n",
    "        bin_lower_score = bin_med_score + bin_posts['score'].std()\n",
    "        bin_count = bin_posts['score'].count().item()\n",
    "        bin_results['bin'].append(t)\n",
    "        bin_results['med'].append(bin_med_score)\n",
    "#         bin_results['upper'].append(bin_upper_score)\n",
    "#         bin_results['lower'].append(bin_lower_score)\n",
    "        bin_results['count'].append(bin_count)\n",
    "    \n",
    "    return bin_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla = [1,2,3]\n",
    "za = ['a', 'b', 'c']\n",
    "\n",
    "i = pd.Series(bla, index=za).index\n",
    "v = pd.Series(bla, index=za).values\n",
    "\n",
    "v.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def to_message(row):\n",
    "    is_good = row['good']\n",
    "    words = \"{0} {1}\".format(row['title'], row['selftext'])\n",
    "    return (words, is_good)\n",
    "\n",
    "def classify_posts(lower, upper, score):\n",
    "    if score >= upper:\n",
    "        return True\n",
    "    elif score < lower:\n",
    "        return False\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def probs_to_distance(p_good, p_bad):\n",
    "    return (p_good - p_bad)/math.sqrt(2)\n",
    "\n",
    "def get_word_probs(posts):\n",
    "    result = {\n",
    "        \"best\": {\"words\": [], \"probs\": []}, \"worst\": {\"words\": [], \"probs\": []}\n",
    "    }\n",
    "#     lower_score = max([posts['score'].quantile(0.33), 2])\n",
    "    lower_score = 3\n",
    "    upper_score = max([posts['score'].quantile(0.66), 4])\n",
    "    posts = posts.assign(\n",
    "        good=posts['score'].apply(\n",
    "            lambda p: classify_posts(lower_score, upper_score, p)\n",
    "        )).dropna(subset=['good'])\n",
    "    messages = [to_message(post) for _, post in posts.iterrows()]\n",
    "    word_probs = messages_to_word_probabilities(messages)\n",
    "    word_dists = [(word, probs_to_distance(p_good, p_bad)) for\n",
    "                  word, p_good, p_bad in word_probs]\n",
    "\n",
    "    if len(word_dists) == 0:\n",
    "        return result\n",
    "    \n",
    "    words, dists = zip(*word_dists)\n",
    "    word_dists = pd.Series(dists, index=words).sort_values()\n",
    "    worst = word_dists.head(10)\n",
    "    best = word_dists.tail(10).sort_values(ascending=False)\n",
    "    result['best']['words'] = best.index.tolist()\n",
    "    result['best']['probs'] = best.values.tolist()\n",
    "    result['worst']['words'] = worst.index.tolist()\n",
    "    result['worst']['probs'] = worst.values.tolist()\n",
    "    return result\n",
    "\n",
    "    \n",
    "def tokenize(message):\n",
    "    stops = ['our', 'their', 'your', 'one', 'about', \"i'm\", 'up', 'out', 'am', 'any', 'like', 'when', 'now', 'her', \"it's\",  'we', 'us', 'they', 'he', 'she', 'his', 'u', 'com', 'http', 'https', 'www', 'or', 'who', 'would', 'had', 'any' 'at', 'got', 'l', 'my', 'me', 'does','get', 'were', 'what', 'at', 'too', 'as', 's', 'an', 'than', 'do', 'so', 'no', 'it', 'how', 'be', 'has',  'a','can', 'will', 'have', 'if', 'why', 'but','he', 'the', 'and', 'on', 'all', 'is', 'emptystring', 'are', 'said', 'in', 'of', 'just', 'that', 'i', 'with', 'was', 'r', 't', 'to', 'for', 'by', 'you', 'there', 'not', 'to', 'from', 'this', '0' , '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    filt_words = list(filter(lambda x: x not in stops, all_words))\n",
    "    \n",
    "    return set(filt_words)   \n",
    "    \n",
    "def count_words(messages):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_good in messages:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_good else 1] += 1\n",
    "    return counts\n",
    "    \n",
    "def messages_to_word_probabilities(messages):\n",
    "    num_goodposts = len([is_good\n",
    "                     for message, is_good in messages\n",
    "                     if is_good])\n",
    "    num_non_goodposts = len(messages) - num_goodposts\n",
    "\n",
    "    k = 0.5\n",
    "    word_counts = count_words(messages)\n",
    "    word_probs = word_probabilities(word_counts,\n",
    "                                         num_goodposts,\n",
    "                                         num_non_goodposts,\n",
    "                                         k)\n",
    "    return word_probs\n",
    "\n",
    "def word_probabilities(counts, total_goodposts, total_non_goodposts, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets\n",
    "    w, p(w | good) and p(w | ~good)\"\"\"\n",
    "    return [(w,\n",
    "             (good + k) / (total_goodposts + 2 * k),\n",
    "             (non_good + k) / (total_non_goodposts + 2 * k))\n",
    "             for w, (good, non_good) in counts.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
